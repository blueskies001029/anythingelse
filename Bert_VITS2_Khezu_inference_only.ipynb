{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bert-VITS2 2.3 전용 추론 코랩\n",
        "다른 VITS 계열 추론 안 됨\n",
        "\n",
        "다른 버전(2.2, 2.1등) 추론 안 해봄\n",
        "\n",
        "G.pth과 json 파일이 필수입니다\n",
        "\n",
        "## **※※※CPU에서도 가능※※※**\n",
        "\n",
        "\n",
        "> 코랩은 정책상 WebUI를 이용하는 것을 금지하고 있습니다. 이 코랩을 이용하는 모든 이용자는, 이 코랩을 이용하는 것으로 인해 **코랩 영구 사용 불가** 등의 불이익을 당할 수도 있다는 사실을 인지하였다고 간주합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZwyOqiMquP7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 준비\n",
        "!git clone https://github.com/fishaudio/Bert-VITS2.git\n",
        "#!git checkout 87462fe50f3f79ebf77600bac89548f192352d0c\n",
        "#!python -m venv venv\n",
        "#!venv\\Scripts\\activate\n",
        "\n",
        "%cd Bert-VITS2\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "#!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "!pip install -r requirements.txt\n",
        "#!pip install --upgrade pip setuptools numpy numba\n",
        "#!pip uninstall tensorflow\n",
        "#!pip install tensorflow[and-cuda] --upgrade --force-reinstall\n",
        "\n",
        "import os.path\n",
        "from os import path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjNUPOO3yksP",
        "outputId": "ac2a3e83-6914-456a-b849-ac010fa23162",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Bert-VITS2'...\n",
            "remote: Enumerating objects: 2396, done.\u001b[K\n",
            "remote: Counting objects: 100% (224/224), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 2396 (delta 114), reused 148 (delta 77), pack-reused 2172\u001b[K\n",
            "Receiving objects: 100% (2396/2396), 9.61 MiB | 8.72 MiB/s, done.\n",
            "Resolving deltas: 100% (1402/1402), done.\n",
            "/content/Bert-VITS2\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting librosa==0.9.2 (from -r requirements.txt (line 1))\n",
            "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.23.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.58.1)\n",
            "Collecting phonemizer (from -r requirements.txt (line 5))\n",
            "  Downloading phonemizer-3.2.1-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.11.4)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.15.1)\n",
            "Collecting Unidecode (from -r requirements.txt (line 8))\n",
            "  Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting amfm_decompy (from -r requirements.txt (line 9))\n",
            "  Downloading AMFM_decompy-1.0.11.tar.gz (751 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.5/751.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.42.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (4.35.2)\n",
            "Collecting pypinyin (from -r requirements.txt (line 12))\n",
            "  Downloading pypinyin-0.50.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cn2an (from -r requirements.txt (line 13))\n",
            "  Downloading cn2an-0.5.22-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio==3.50.2 (from -r requirements.txt (line 14))\n",
            "  Downloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting av (from -r requirements.txt (line 15))\n",
            "  Downloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mecab-python3 (from -r requirements.txt (line 16))\n",
            "  Downloading mecab_python3-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.7/581.7 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting loguru (from -r requirements.txt (line 17))\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unidic-lite (from -r requirements.txt (line 18))\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cmudict (from -r requirements.txt (line 19))\n",
            "  Downloading cmudict-1.0.16-py3-none-any.whl (939 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fugashi (from -r requirements.txt (line 20))\n",
            "  Downloading fugashi-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (600 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m600.9/600.9 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting num2words (from -r requirements.txt (line 21))\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (2.31.0)\n",
            "Collecting pyopenjtalk-prebuilt (from -r requirements.txt (line 24))\n",
            "  Downloading pyopenjtalk_prebuilt-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaconv (from -r requirements.txt (line 25))\n",
            "  Downloading jaconv-0.3.4.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (5.9.5)\n",
            "Collecting GPUtil (from -r requirements.txt (line 27))\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting vector_quantize_pytorch (from -r requirements.txt (line 28))\n",
            "  Downloading vector_quantize_pytorch-1.12.5-py3-none-any.whl (24 kB)\n",
            "Collecting g2p_en (from -r requirements.txt (line 29))\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from -r requirements.txt (line 30))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pykakasi (from -r requirements.txt (line 31))\n",
            "  Downloading pykakasi-2.2.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langid (from -r requirements.txt (line 32))\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (4.4.2)\n",
            "Collecting resampy>=0.2.2 (from librosa==0.9.2->-r requirements.txt (line 1))\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (23.2)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (4.2.2)\n",
            "Collecting fastapi (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading fastapi-0.105.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.6.1 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (0.19.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (2.1.3)\n",
            "Collecting orjson~=3.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (1.10.13)\n",
            "Collecting pydub (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (4.5.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.6.1->gradio==3.50.2->-r requirements.txt (line 14)) (2023.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->-r requirements.txt (line 4)) (0.41.1)\n",
            "Collecting segments (from phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading segments-2.2.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.10/dist-packages (from phonemizer->-r requirements.txt (line 5)) (23.1.0)\n",
            "Collecting dlinfo (from phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading dlinfo-1.2.1-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.5.1)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 11)) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 11)) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 11)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 11)) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 11)) (4.66.1)\n",
            "Collecting proces>=0.1.3 (from cn2an->-r requirements.txt (line 13))\n",
            "  Downloading proces-0.1.7-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict->-r requirements.txt (line 19)) (7.0.0)\n",
            "Collecting docopt>=0.6.2 (from num2words->-r requirements.txt (line 21))\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 23)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 23)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 23)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 23)) (2023.11.17)\n",
            "Requirement already satisfied: cython>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pyopenjtalk-prebuilt->-r requirements.txt (line 24)) (3.0.6)\n",
            "Collecting einops>=0.7.0 (from vector_quantize_pytorch->-r requirements.txt (line 28))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from vector_quantize_pytorch->-r requirements.txt (line 28)) (2.1.0+cu121)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from g2p_en->-r requirements.txt (line 29)) (3.8.1)\n",
            "Requirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from g2p_en->-r requirements.txt (line 29)) (7.0.0)\n",
            "Collecting distance>=0.1.3 (from g2p_en->-r requirements.txt (line 29))\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated (from pykakasi->-r requirements.txt (line 31))\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (0.12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=5->cmudict->-r requirements.txt (line 19)) (3.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p_en->-r requirements.txt (line 29)) (8.1.7)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==3.50.2->-r requirements.txt (line 14)) (2023.3.post1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa==0.9.2->-r requirements.txt (line 1)) (4.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->librosa==0.9.2->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 1)) (1.16.0)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->pykakasi->-r requirements.txt (line 31)) (1.14.1)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.50.2->-r requirements.txt (line 14)) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions~=4.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting httpcore==1.* (from httpx->gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.50.2->-r requirements.txt (line 14)) (1.3.0)\n",
            "Collecting clldutils>=1.7.3 (from segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading clldutils-3.22.1-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting csvw>=1.5.6 (from segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading csvw-3.2.1-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->vector_quantize_pytorch->-r requirements.txt (line 28)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->vector_quantize_pytorch->-r requirements.txt (line 28)) (3.2.1)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->vector_quantize_pytorch->-r requirements.txt (line 28)) (2.1.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio==3.50.2->-r requirements.txt (line 14)) (1.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 1)) (2.21)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer->-r requirements.txt (line 5)) (0.9.0)\n",
            "Collecting colorlog (from clldutils>=1.7.3->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
            "Collecting bibtexparser>=2.0.0b4 (from clldutils>=1.7.3->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading bibtexparser-2.0.0b4-py3-none-any.whl (37 kB)\n",
            "Collecting pylatexenc (from clldutils>=1.7.3->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer->-r requirements.txt (line 5)) (4.9.3)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5)) (2.14.0)\n",
            "Collecting colorama (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting isodate (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting language-tags (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading language_tags-1.2.0-py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.4/213.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdflib (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2 (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5)) (4.1.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (0.15.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 7)) (3.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->vector_quantize_pytorch->-r requirements.txt (line 28)) (1.3.0)\n",
            "Building wheels for collected packages: amfm_decompy, unidic-lite, jaconv, GPUtil, langid, distance, docopt, ffmpy, pylatexenc\n",
            "  Building wheel for amfm_decompy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for amfm_decompy: filename=AMFM_decompy-1.0.11-py3-none-any.whl size=42835 sha256=d2bb47c67c9bed49df88654a93c603649939e4827c9d492ed1a418a692aea352\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/81/e7/443ad333f2f4ed8c06fc027caeb0d0c84b896fe7e56c2e92b1\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658817 sha256=8f8b571c1c599fbe9467fe0b5cd435ef6fcbd7c471c80df115b316f426b554b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.3.4-py3-none-any.whl size=16415 sha256=d40b086dcebbbab1d2bf46373eb9554e118ddcf3086ad81b2819016d27a2004e\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/8f/2e/a730bf1fca05b33e532d5d91dabdf406c9b718ec85b01b1b54\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7393 sha256=41ee9e285292c043f184f067351144a7d7f00373ce5d09a592e20cd152545678\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941172 sha256=f80c3620913f8de45978dd385a82f3a95e418a8d37b92f89e61e7336d8616ba9\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=d3b7586e484ba2c706931083199182f9cd286dfaac7f90af667812fb550c757f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=0a3ef8e832903219a4ad97d3871de8a7889fa26ca5c60c65087e331770166306\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=da5d4012462390d83b35aa8df5d6ae47ee16c1c443a580bdb514ac8db850d500\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=8815f3e47d14c3973edb60840988d2c8d0ab7a76546c63b4ff6144a6a4b7861a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/31/8b/e09b0386afd80cfc556c00408c9aeea5c35c4d484a9c762fd5\n",
            "Successfully built amfm_decompy unidic-lite jaconv GPUtil langid distance docopt ffmpy pylatexenc\n",
            "Installing collected packages: unidic-lite, sentencepiece, rfc3986, pylatexenc, pydub, mecab-python3, language-tags, jaconv, GPUtil, ffmpy, docopt, dlinfo, distance, websockets, Unidecode, typing-extensions, semantic-version, python-multipart, pypinyin, pyopenjtalk-prebuilt, proces, orjson, num2words, loguru, langid, isodate, h11, fugashi, einops, deprecated, colorlog, colorama, bibtexparser, av, aiofiles, uvicorn, starlette, resampy, rdflib, pykakasi, httpcore, cn2an, cmudict, clldutils, amfm_decompy, vector_quantize_pytorch, librosa, httpx, fastapi, gradio-client, g2p_en, csvw, segments, gradio, phonemizer\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.1\n",
            "    Uninstalling librosa-0.10.1:\n",
            "      Successfully uninstalled librosa-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GPUtil-1.4.0 Unidecode-1.3.7 aiofiles-23.2.1 amfm_decompy-1.0.11 av-11.0.0 bibtexparser-2.0.0b4 clldutils-3.22.1 cmudict-1.0.16 cn2an-0.5.22 colorama-0.4.6 colorlog-6.8.0 csvw-3.2.1 deprecated-1.2.14 distance-0.1.3 dlinfo-1.2.1 docopt-0.6.2 einops-0.7.0 fastapi-0.105.0 ffmpy-0.3.1 fugashi-1.3.0 g2p_en-2.1.0 gradio-3.50.2 gradio-client-0.6.1 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 isodate-0.6.1 jaconv-0.3.4 langid-1.1.6 language-tags-1.2.0 librosa-0.9.2 loguru-0.7.2 mecab-python3-1.0.8 num2words-0.5.13 orjson-3.9.10 phonemizer-3.2.1 proces-0.1.7 pydub-0.25.1 pykakasi-2.2.1 pylatexenc-2.10 pyopenjtalk-prebuilt-0.3.0 pypinyin-0.50.0 python-multipart-0.0.6 rdflib-7.0.0 resampy-0.4.2 rfc3986-1.5.0 segments-2.2.1 semantic-version-2.10.0 sentencepiece-0.1.99 starlette-0.27.0 typing-extensions-4.9.0 unidic-lite-1.0.8 uvicorn-0.25.0 vector_quantize_pytorch-1.12.5 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 드라이브 마운트\n",
        "#@markdown 자신의 드라이브에서 모델을 다운로드할 경우 런타임당 1회 실행합니다. ('/content/drive/...')\n",
        "\n",
        "#@markdown 자신의 드라이브라고 해도 공유 링크를 사용하여 모델을 다운로드할 경우에는 필요 없습니다. ('drive.google.com...')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-7Ndi4tOCyAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "c55e200e-ff3e-43aa-e5ae-3c54f00f9ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 모델 로딩\n",
        "\n",
        "#@markdown ###한 번에 여러 모델을 다운받아 사용할 수 없습니다.\n",
        "#@markdown 한 런타임 당 하나의 모델만 사용해주세요\n",
        "\n",
        "#@markdown Google Drive 링크 또는 Hugging Face 링크에서의 다운로드만을 지원합니다.\n",
        "\n",
        "#@markdown WebUI를 껐다가 다른 모델을 불러오기 위해서는 아래 clean_model_folder를 체크하여 이전 모델을 삭제하여주세요.\n",
        "\n",
        "clean_model_folder = True # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import urllib.request\n",
        "import gdown\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import urllib.parse\n",
        "from google.oauth2.service_account import Credentials\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from requests import get\n",
        "import hashlib\n",
        "\n",
        "\n",
        "\n",
        "inference_arrive = '/content/Bert-VITS2/Data/models/'\n",
        "inference_json_arrive = '/content/Bert-VITS2/Data/'\n",
        "os.makedirs(inference_json_arrive, exist_ok=True)\n",
        "dir_name = inference_arrive.split('/')[-1]\n",
        "\n",
        "G_PTH_WHERE = \"/content/drive/MyDrive/Bert-VITS2_Khezu/Miyu/models/G_6000.pth\" # @param {type:\"string\"}\n",
        "SPEAKER_JSON_WHERE = \"/content/drive/MyDrive/Bert-VITS2_Khezu/Miyu/config.json\" # @param {type:\"string\"}\n",
        "\n",
        "if clean_model_folder is True:\n",
        "  if os.path.exists(inference_arrive):\n",
        "    shutil.rmtree(inference_arrive)\n",
        "    print(f'[-] 폴더를 청소하였습니다! 새로운 모델을 다운받아서 사용할 수 있습니다.')\n",
        "\n",
        "os.makedirs(inference_arrive, exist_ok=True)\n",
        "\n",
        "#%cd inference_folder\n",
        "\n",
        "def download_model(url, dir_name):\n",
        "    try:\n",
        "        #extraction_folder = os.path.join(rvc_models_dir, dir_name)\n",
        "\n",
        "        download_name = url.split('/')[-1]\n",
        "        if '?download=true' in download_name:\n",
        "          download_name = download_name.split('?')[-2]\n",
        "        #download_where = '/inference_folder/' +  url.split('/')[-1]\n",
        "        download_where = inference_arrive +  download_name\n",
        "\n",
        "        print(f'[~] {download_name}를 허깅페이스에서 다운받고 있어요...')\n",
        "\n",
        "\n",
        "        #urllib.request.urlretrieve(url, download_where)\n",
        "        #urllib.request.urlretrieve(url, '/content/VITS-fast-fine-tuning/inference_folder')\n",
        "\n",
        "        #print('[~] Extracting zip...')\n",
        "        #extract_zip(extraction_folder, download_where)\n",
        "        if \"/blob/\" in url:\n",
        "          url = url.replace(\"blob\", \"resolve\")\n",
        "          print(\"Resolved URL:\", url)  # Print the resolved URL\n",
        "          !wget \"$url\" -O \"$download_where\"\n",
        "          #/content/VITS-fast-fine-tuning/inference_folder\n",
        "        else:\n",
        "          !wget \"$url\" -O \"$download_where\"\n",
        "          #/content/VITS-fast-fine-tuning/inference_folder\n",
        "          #urllib.request.urlretrieve(url, '/content/VITS-fast-fine-tuning/inference_folder/')\n",
        "        print(f'[+] {download_name}를 허깅페이스에서 성공적으로 다운로드 했어요!')\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(str(e))\n",
        "\n",
        "def download_online_model_drive(file_id, dir_name):\n",
        "    try:\n",
        "        download_where = inference_arrive + '/'\n",
        "\n",
        "        print(f'[~] 링크 공유된 구글드라이브에서 파일을 다운받고 있어요...')\n",
        "\n",
        "        gdown.download(f'https://drive.google.com/uc?id={file_id}', download_where, quiet=False)\n",
        "\n",
        "        #downloaded_where = max(glob(os.path.join(download_where, \"*\")), key=os.path.getctime)\n",
        "        #download_where = downloaded_where.split('/')[-1]\n",
        "        #print(f'[+] {download_name}를 링크 공유된 구글드라이브에서 성공적으로 다운로드 했어요!')\n",
        "        print(f'[+] 링크 공유된 구글드라이브에서 파일을 성공적으로 다운로드 했어요!')\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(str(e))\n",
        "def download_from_your_drive(url, dir_name):\n",
        "    try:\n",
        "        download_name = url.split('/')[-1]\n",
        "        download_where = inference_arrive + '/' +  url.split('/')[-1]\n",
        "\n",
        "        print(f'[~] {download_name}를 마운트 된 구글드라이브에서 다운받고 있어요...')\n",
        "\n",
        "        !cp {url} {download_where}\n",
        "\n",
        "        print(f'[+] {download_name}를 마운트 된 구글드라이브에서 성공적으로 다운로드 했어요!')\n",
        "    except Exception as e:\n",
        "        raise Exception(str(e))\n",
        "\n",
        "# Detecta si la URL es de Google Drive o Hugging Face\n",
        "if 'drive.google.com' in G_PTH_WHERE:\n",
        "    file_id_drive = G_PTH_WHERE.split('/')[-2]\n",
        "    download_online_model_drive(file_id_drive, dir_name)\n",
        "elif 'huggingface.co' in G_PTH_WHERE:\n",
        "    download_model(G_PTH_WHERE, dir_name)\n",
        "elif '/content/drive' in G_PTH_WHERE:\n",
        "  download_from_your_drive(G_PTH_WHERE, dir_name)\n",
        "else:\n",
        "    print('모르는 URL이네요... G_PTH_WHERE가 Google Drive 링크 또는 Hugging Face 링크에서만 다운받을 수 있어요!')\n",
        "\n",
        "if 'drive.google.com' in SPEAKER_JSON_WHERE:\n",
        "    file_id_drive = SPEAKER_JSON_WHERE.split('/')[-2]\n",
        "    download_online_model_drive(file_id_drive, dir_name)\n",
        "elif 'huggingface.co' in SPEAKER_JSON_WHERE:\n",
        "    download_model(SPEAKER_JSON_WHERE, dir_name)\n",
        "elif '/content/drive' in SPEAKER_JSON_WHERE:\n",
        "  download_from_your_drive(SPEAKER_JSON_WHERE, dir_name)\n",
        "else:\n",
        "    print('모르는 URL이네요... SPEAKER_JSON_WHERE가 Google Drive 링크 또는 Hugging Face 링크에서만 다운받을 수 있어요!')\n",
        "\n",
        "\n",
        "#%cd ..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJuQtpOKxSkK",
        "outputId": "082b1b5c-6869-4f8a-f73b-59adad78cc24",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-] 폴더를 청소하였습니다! 새로운 모델을 다운받아서 사용할 수 있습니다.\n",
            "[~] G_6000.pth를 마운트 된 구글드라이브에서 다운받고 있어요...\n",
            "[+] G_6000.pth를 마운트 된 구글드라이브에서 성공적으로 다운로드 했어요!\n",
            "[~] config.json를 마운트 된 구글드라이브에서 다운받고 있어요...\n",
            "[+] config.json를 마운트 된 구글드라이브에서 성공적으로 다운로드 했어요!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 다운로드2\n",
        "!python preprocess_text.py\n",
        "!python preprocess_text.py\n",
        "#2.1\n",
        "!curl -L -o /content/Bert-VITS2/emotional/wav2vec2-large-robust-12-ft-emotion-msp-dim/pytorch_model.bin https://huggingface.co/audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim/resolve/main/pytorch_model.bin\n",
        "#2.2\n",
        "!curl -L -o /content/Bert-VITS2/emotional/clap-htsat-fused/pytorch_model.bin https://huggingface.co/laion/clap-htsat-fused/resolve/main/pytorch_model.bin\n",
        "#2.3\n",
        "!curl -L -o slm/wavlm-base-plus/pytorch_model.bin https://huggingface.co/microsoft/wavlm-base-plus/resolve/main/pytorch_model.bin\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "K26oYJCEV4Te",
        "outputId": "47324346-6634-4a94-ef6b-18eb83483cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "已根据默认配置文件default_config.yml生成配置文件config.yml。请按该配置文件的说明进行配置后重新运行。\n",
            "如无特殊需求，请勿修改default_config.yml或备份该文件。\n",
            "pytorch_model.bin: 100% 1.32G/1.32G [00:09<00:00, 139MB/s]\n",
            "pytorch_model.bin: 100% 1.31G/1.31G [00:09<00:00, 138MB/s]\n",
            "spm.model: 100% 2.46M/2.46M [00:00<00:00, 161MB/s]\n",
            "pytorch_model.bin: 100% 874M/874M [00:08<00:00, 100MB/s]\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "Usage: preprocess_text.py [OPTIONS]\n",
            "Try 'preprocess_text.py --help' for help.\n",
            "\n",
            "Error: Invalid value for '--transcription-path': File 'Data/filelists/你的数据集文本.list' does not exist.\n",
            "\u001b[0m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1093  100  1093    0     0   4870      0 --:--:-- --:--:-- --:--:--  4857\n",
            "100  360M  100  360M    0     0   213M      0  0:00:01  0:00:01 --:--:--  285M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title yaml 변경 (아마 실행할 필요 없음)\n",
        "\n",
        "import yaml\n",
        "with open('/content/Bert-VITS2/config.yml') as f:\n",
        "  config = yaml.load(f, Loader=yaml.FullLoader)\n",
        "#config['dataset_path'] = \"Data/\" + model_name + '/'\n",
        "config['dataset_path'] = \"Data/\"\n",
        "config['preprocess_text']['transcription_path'] = 'filelists/text.list'\n",
        "#config['train_ms']['keep_ckpts'] = -1 #모든 모델 저장\n",
        "#config['server']['models']['language'] = 'JP'\n",
        "'''\n",
        "config['dataset_path'] = \"\" #특정 경로 지정시 모든 경로는 상대적으로 입력되어야함. 아닐 경우 루트 기준.\n",
        "#yml의 대부분이 상대경로 기준이므로, 로컬 학습시는 dataset_path를 \"Data/model_name/\"로 바꾸고 일부만 바꿔서 사용하는 게 편함\n",
        "#난 상대경로 쓰는 법 몰라!\n",
        "config['preprocess_text']['transcription_path'] = txt_file_arrive_where\n",
        "resample:in_dir:'/Data/'+model_name+'/'+\"audios/raw\"\n",
        "resample:out_dir:'/Data/'+model_name+'/'+\"audios/wavs\"\n",
        "preprocess_text:transcription_path:'/Data/'+model_name+'/'+\"filelists/text.list\"\n",
        "preprocess_text:train_path:'/Data/'+model_name+'/'+\"filelists/train.list\n",
        "preprocess_text:val_path:'/Data/'+model_name+'/'+\"filelists/val.list\n",
        "preprocess_text:config_path:'/Data/'+model_name+'/'+\"config.json\"\n",
        "bert_gen:config_path:'/Data/'+model_name+'/'+\"config.json\"\n",
        "emo_gen:config_path: '/Data/'+model_name+'/'+\"config.json\"\n",
        "train_ms: model: '/Data/'+model_name+'/'+\"models\"\n",
        "train_ms: config_path: '/Data/'+model_name+'/'+\"config.json\"\n",
        "'''\n",
        "\n",
        "with open('/content/Bert-VITS2/config.yml', 'w') as f:\n",
        "  yaml.dump(config, f)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4G9R7_msV5xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title .json 변경 (업로드했으면 실행 X) only for 2.3\n",
        "import shutil\n",
        "json_where = '/content/Bert-VITS2/Data/models/config.json'\n",
        "shutil.copy('/content/Bert-VITS2/configs/config.json', json_where)\n",
        "\n",
        "import json\n",
        "with open(json_where, 'r') as f:\n",
        "    json_data = json.load(f)\n",
        "#print(json.dumps(json_data) )\n",
        "json_data['train']['batch_size'] = 6 #기본값12\n",
        "json_data['train']['log_interval'] = 50 #기본값 200\n",
        "#json_data['train']['eval_interval'] = 500 #기본값 1000\n",
        "with open(json_where, 'w', encoding='utf-8') as make_file:\n",
        "    json.dump(json_data, make_file, indent=\"\\t\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "cellView": "form",
        "id": "4k6htz19WauJ",
        "outputId": "1971919c-4786-4f1c-b7b1-05fe22480625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-13b8d31d4b84>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mjson_where\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/Bert-VITS2/models/config.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Bert-VITS2/configs/config.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_where\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                     \u001b[0;31m# macOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Bert-VITS2/models/config.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### WebUI 실행\n",
        "##!python webui.py\n",
        "\n",
        "import yaml\n",
        "import json\n",
        "from glob import glob\n",
        "import shutil\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "G_name_please = \"\" # @param {type:\"string\"}\n",
        "inference_mode = \"cpu\" # @param [\"cuda\", \"cpu\"]\n",
        "models_where = inference_arrive\n",
        "latest_ckpt_name = \"\"\n",
        "\n",
        "if G_name_please is \"\":\n",
        "  latest_ckpt_G=max(glob(os.path.join(models_where, \"*.pth\")), key=os.path.getctime)\n",
        "  #max(glob(os.path.join(models_where, \"G_*.pth\")), key=os.path.getctime)\n",
        "  latest_ckpt_G_name = latest_ckpt_G.split('/')[-1]\n",
        "else:\n",
        "  latest_ckpt_G_name = G_name_please\n",
        "\n",
        "latest_json = max(glob(os.path.join(models_where, \"*.json\")), key=os.path.getctime)\n",
        "latest_json_name = latest_json.split('/')[-1]\n",
        "\n",
        "\n",
        "with open('/content/Bert-VITS2/config.yml') as f:\n",
        "  config = yaml.load(f, Loader=yaml.FullLoader)\n",
        "config['dataset_path'] = \"Data/\"\n",
        "#config['webui']['device'] = \"cuda\"  # cpu나 cuda. cpu로도 아무리 늦어도 1분 내로 생성. GPU로 켰다면 cuda로 변경할 것\n",
        "#config['webui']['device'] = \"cpu\"\n",
        "config['webui']['device'] = inference_mode\n",
        "config['webui']['model'] = \"models/\" + latest_ckpt_G_name\n",
        "config['webui']['share'] = \"true\"\n",
        "#config['webui']['port'] = 7860\n",
        "config['webui']['debug'] = \"false\"\n",
        "config['webui']['config_path'] = \"models/\" + latest_json_name\n",
        "#config['server']['models']['device'] = lllist\n",
        "with open('/content/Bert-VITS2/config.yml', 'w') as f:\n",
        "    yaml.dump(config, f)\n",
        "\n",
        "'''\n",
        "with open(latest_json, 'r') as f:\n",
        "    json_data = json.load(f)\n",
        "#json_data['train']['eval_interval'] = 500 #기본값 1000\n",
        "with open(latest_json, 'w', encoding='utf-8') as make_file:\n",
        "    json.dump(json_data, make_file, indent=\"\\t\")\n",
        "'''\n",
        "\n",
        "%mv webui.py web_inference.py\n",
        "!python web_inference.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-XLGHVkztoz",
        "outputId": "884b6db6-e79e-45b4-f6c9-f4d15cd6c265",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'webui.py': No such file or directory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:16: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:16: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<ipython-input-23-2330a0330b62>:16: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if G_name_please is \"\":\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| numexpr.utils | INFO | NumExpr defaulting to 2 threads.\n",
            "| __main__ | INFO | Enable DEBUG-LEVEL log\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "| utils | INFO | Loaded checkpoint 'Data/models/G_6000.pth' (iteration 170)\n",
            "推理页面已开启!\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://b86e2a96b2a4ef4a1a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "Building prefix dict from the default dictionary ...\n",
            "| jieba | DEBUG | Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "| jieba | DEBUG | Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.838 seconds.\n",
            "| jieba | DEBUG | Loading model cost 0.838 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "| jieba | DEBUG | Prefix dict has been built successfully.\n",
            "Some weights of the model checkpoint at ./bert/chinese-roberta-wwm-ext-large were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py:183: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://b86e2a96b2a4ef4a1a.gradio.live\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MxMxUsE9zwDT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}